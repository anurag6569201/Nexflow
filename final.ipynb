{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 1 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 2 orphan components.\n",
      "LOG (VoskAPI:Collapse():nnet-utils.cc:1488) Added 1 components, removed 2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-small-en-in-0.4/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from vosk-model-small-en-in-0.4/graph/HCLr.fst vosk-model-small-en-in-0.4/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo vosk-model-small-en-in-0.4/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for your command...\n",
      "Live Transcription: what are you doing"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m last_audio_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Track when the last audio chunk was received\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Read audio data from the stream\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes_per_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/LLM/llmenv/lib/python3.12/site-packages/pyaudio/__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[0;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from vosk import Model, KaldiRecognizer\n",
    "import pyaudio\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# Initialize the Vosk model and recognizer\n",
    "model = Model('vosk-model-small-en-in-0.4')\n",
    "recognizer = KaldiRecognizer(model, 16000)\n",
    "\n",
    "# Set up PyAudio for audio input\n",
    "cap = pyaudio.PyAudio()\n",
    "\n",
    "# Parameters for audio stream\n",
    "channels = 1\n",
    "rate = 16000\n",
    "frames_per_buffer = 8192\n",
    "\n",
    "# Start the audio stream\n",
    "stream = cap.open(format=pyaudio.paInt16, channels=channels, rate=rate,\n",
    "                  input=True, frames_per_buffer=frames_per_buffer)\n",
    "print(\"Listening for your command...\")\n",
    "\n",
    "def llm_response(text):\n",
    "    load_dotenv()\n",
    "    google_gemini_api = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "    # Initialize the LLM model with API key\n",
    "    llm_model = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=google_gemini_api)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    you are anuradha a girl you have to answer like a girl make sure\n",
    "    1. Clear and concise.\n",
    "    2. Conversational and easy to understand.\n",
    "    3. Suitable for text-to-speech conversion without any awkward phrasing or technical jargon.\n",
    "    Here is the user's query: \n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict the response from LLM\n",
    "    llm_response = llm_model.predict(prompt)\n",
    "    return llm_response\n",
    "\n",
    "\n",
    "def genrate_voice(llm_output_for_voice):\n",
    "    tts = gTTS(text=llm_output_for_voice, lang='en', slow=False,tld='co.in',lang_check=True)\n",
    "    tts.save(\"temp.mp3\")\n",
    "    playsound.playsound(\"temp.mp3\")\n",
    "    os.remove(\"temp.mp3\")\n",
    "    \n",
    "# Initialize variables\n",
    "commands = []  # List to store completed commands\n",
    "pause_duration = 2  # Pause duration in seconds to consider as finished\n",
    "last_audio_time = time.time()  # Track when the last audio chunk was received\n",
    "\n",
    "while True:\n",
    "    # Read audio data from the stream\n",
    "    data = stream.read(frames_per_buffer)\n",
    "\n",
    "    if len(data) == 0:\n",
    "        continue\n",
    "\n",
    "    # Process the audio data with Vosk recognizer\n",
    "    if recognizer.AcceptWaveform(data):\n",
    "        result = recognizer.Result()\n",
    "        result_dict = json.loads(result)\n",
    "        text = result_dict.get('text', '')\n",
    "\n",
    "        if text:\n",
    "            # Append the recognized text directly to the commands list\n",
    "            commands.append(text)\n",
    "            \n",
    "            # Show the latest transcription live (latest command)\n",
    "            print(f\"\\rLive Transcription: {commands[-1]}\", end=\"\")\n",
    "\n",
    "    # Check if there's been a pause of 2-3 seconds\n",
    "    if time.time() - last_audio_time > pause_duration:\n",
    "        # If there's a pause, finalize the transcription and print all commands so far\n",
    "        if commands:  # Only process if there are any commands\n",
    "            print(\"\\nCompleted Commands:\")\n",
    "            for cmd in commands:\n",
    "                print(f\"- {cmd}\")\n",
    "            \n",
    "            # Print the latest command (the most recent transcription)\n",
    "            print(f\"\\nLatest Command: {commands[-1]}\")\n",
    "            llm_output=llm_response(commands[-1])\n",
    "            genrate_voice(llm_output)\n",
    "\n",
    "            # Reset for the next command\n",
    "            print(\"Listening for the next command...\")\n",
    "        \n",
    "        # Reset the last audio time to track when the next audio starts\n",
    "        last_audio_time = time.time()\n",
    "\n",
    "    # Update last audio time if new data is received\n",
    "    last_audio_time = time.time()\n",
    "\n",
    "# Stop the stream and close the PyAudio session\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "cap.terminate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what are you doing today',\n",
       " 'okay',\n",
       " 'oh cavett night',\n",
       " 'not listening very well']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import playsound\n",
    "\n",
    "# Your text to convert to speech\n",
    "text = \"Hello, this is a test of Google Text-to-Speech.\"\n",
    "\n",
    "# Language in which you want to convert\n",
    "language = 'en'  # English\n",
    "\n",
    "# Passing the text and language to the engine, slow=False makes the speech faster\n",
    "tts = gTTS(text=text, lang=language, slow=False,tld='co.in',lang_check=True)\n",
    "\n",
    "# Saving the speech to a temporary file and playing it\n",
    "tts.save(\"temp.mp3\")\n",
    "playsound.playsound(\"temp.mp3\")\n",
    "\n",
    "# Optionally, you can remove the temporary file after playing (use os.remove if needed)\n",
    "import os\n",
    "os.remove(\"temp.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 1 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 2 orphan components.\n",
      "LOG (VoskAPI:Collapse():nnet-utils.cc:1488) Added 1 components, removed 2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-small-en-in-0.4/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from vosk-model-small-en-in-0.4/graph/HCLr.fst vosk-model-small-en-in-0.4/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo vosk-model-small-en-in-0.4/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for your command...\n",
      "Live Transcription: okay exityou doing today"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 70\u001b[0m\n\u001b[1;32m     66\u001b[0m last_audio_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Track when the last audio chunk was received\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Read audio data from the stream\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes_per_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/LLM/llmenv/lib/python3.12/site-packages/pyaudio/__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[0;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from vosk import Model, KaldiRecognizer\n",
    "import pyaudio\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Vosk model and recognizer\n",
    "model = Model('vosk-model-small-en-in-0.4')\n",
    "recognizer = KaldiRecognizer(model, 16000)\n",
    "\n",
    "# Set up PyAudio for audio input\n",
    "cap = pyaudio.PyAudio()\n",
    "\n",
    "# Parameters for audio stream\n",
    "channels = 1\n",
    "rate = 16000\n",
    "frames_per_buffer = 8192\n",
    "\n",
    "# Start the audio stream\n",
    "stream = cap.open(format=pyaudio.paInt16, channels=channels, rate=rate,\n",
    "                  input=True, frames_per_buffer=frames_per_buffer)\n",
    "print(\"Listening for your command...\")\n",
    "\n",
    "# Function to get LLM response\n",
    "def llm_response(text):\n",
    "    google_gemini_api = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not google_gemini_api:\n",
    "        print(\"Error: GOOGLE_API_KEY is not set in environment variables.\")\n",
    "        return \"Sorry, I couldn't get a response right now.\"\n",
    "    \n",
    "    # Initialize the LLM model with API key\n",
    "    llm_model = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=google_gemini_api)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    you are anuradha a girl you have to answer like a girl make sure\n",
    "    1. Clear and concise.\n",
    "    2. Conversational and easy to understand.\n",
    "    3. Suitable for text-to-speech conversion without any awkward phrasing or technical jargon.\n",
    "    Here is the user's query: \n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict the response from LLM\n",
    "    llm_response = llm_model.predict(prompt)\n",
    "    return llm_response\n",
    "\n",
    "# Function to generate voice from LLM output\n",
    "def generate_voice(llm_output_for_voice):\n",
    "    tts = gTTS(text=llm_output_for_voice, lang='en', slow=False, tld='co.in', lang_check=True)\n",
    "    tts.save(\"temp.mp3\")\n",
    "    playsound.playsound(\"temp.mp3\")\n",
    "    os.remove(\"temp.mp3\")\n",
    "\n",
    "# Initialize variables\n",
    "commands = []  # List to store completed commands\n",
    "pause_duration = 2  # Pause duration in seconds to consider as finished\n",
    "last_audio_time = time.time()  # Track when the last audio chunk was received\n",
    "\n",
    "while True:\n",
    "    # Read audio data from the stream\n",
    "    data = stream.read(frames_per_buffer)\n",
    "\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "\n",
    "    # Process the audio data with Vosk recognizer\n",
    "    if recognizer.AcceptWaveform(data):\n",
    "        result = recognizer.Result()\n",
    "        result_dict = json.loads(result)\n",
    "        text = result_dict.get('text', '')\n",
    "\n",
    "        if text:\n",
    "            # Append the recognized text directly to the commands list\n",
    "            commands.append(text)\n",
    "            \n",
    "            # Show the latest transcription live (latest command)\n",
    "            print(f\"\\rLive Transcription: {commands[-1]}\", end=\"\")\n",
    "\n",
    "    # Check if there's been a pause of 2-3 seconds\n",
    "    if time.time() - last_audio_time > pause_duration:\n",
    "        # If there's a pause, finalize the transcription and print all commands so far\n",
    "        if commands:  # Only process if there are any commands\n",
    "            print(\"\\nCompleted Commands:\")\n",
    "            for cmd in commands:\n",
    "                print(f\"- {cmd}\")\n",
    "            \n",
    "            # Print the latest command (the most recent transcription)\n",
    "            print(f\"\\nLatest Command: {commands[-1]}\")\n",
    "            llm_output = llm_response(commands[-1])\n",
    "            generate_voice(llm_output)\n",
    "\n",
    "            # Reset for the next command\n",
    "            print(\"Listening for the next command...\")\n",
    "        \n",
    "        # Reset the last audio time to track when the next audio starts\n",
    "        last_audio_time = time.time()\n",
    "\n",
    "    # Update last_audio_time only when new data is received\n",
    "    if len(data) > 0:\n",
    "        last_audio_time = time.time()\n",
    "\n",
    "# Stop the stream and close the PyAudio session\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "cap.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 1 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 2 orphan components.\n",
      "LOG (VoskAPI:Collapse():nnet-utils.cc:1488) Added 1 components, removed 2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-small-en-in-0.4/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from vosk-model-small-en-in-0.4/graph/HCLr.fst vosk-model-small-en-in-0.4/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo vosk-model-small-en-in-0.4/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for your command...\n",
      "Recognized Command: i wanted to know what is in apple\n",
      "Latest Command: i wanted to know what is in apple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732363340.494008  529347 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1732363340.494577  529347 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1732363340.909605  531193 subchannel.cc:806] subchannel 0x178b66080 {address=ipv6:%5B2404:6800:4009:811::200a%5D:443, args={grpc.client_channel_factory=0x10789dff0, grpc.default_authority=generativelanguage.googleapis.com:443, grpc.dns_enable_srv_queries=1, grpc.http2_scheme=https, grpc.internal.channel_credentials=0x17ae7eaf0, grpc.internal.client_channel_call_destination=0x1387bba18, grpc.internal.event_engine=0x178b62ff0, grpc.internal.security_connector=0x178b64d40, grpc.internal.subchannel_pool=0x14fffc590, grpc.max_receive_message_length=-1, grpc.max_send_message_length=-1, grpc.primary_user_agent=grpc-python/1.65.1, grpc.resource_quota=0x14fff8900, grpc.server_uri=dns:///generativelanguage.googleapis.com:443}}: connect failed (UNKNOWN:connect: No route to host (65) {created_time:\"2024-11-23T17:32:20.909401+05:30\"}), backing off for 1000 ms\n"
     ]
    }
   ],
   "source": [
    "from vosk import Model, KaldiRecognizer\n",
    "import pyaudio\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Vosk model and recognizer\n",
    "model = Model('vosk-model-small-en-in-0.4')\n",
    "recognizer = KaldiRecognizer(model, 16000)\n",
    "\n",
    "# Set up PyAudio for audio input\n",
    "cap = pyaudio.PyAudio()\n",
    "\n",
    "# Parameters for audio stream\n",
    "channels = 1\n",
    "rate = 16000\n",
    "frames_per_buffer = 8192\n",
    "\n",
    "# Start the audio stream\n",
    "stream = cap.open(format=pyaudio.paInt16, channels=channels, rate=rate,\n",
    "                  input=True, frames_per_buffer=frames_per_buffer)\n",
    "print(\"Listening for your command...\")\n",
    "\n",
    "# Function to get LLM response\n",
    "def llm_response(text):\n",
    "    google_gemini_api = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not google_gemini_api:\n",
    "        print(\"Error: GOOGLE_API_KEY is not set in environment variables.\")\n",
    "        return \"Sorry, I couldn't get a response right now.\"\n",
    "    \n",
    "    # Initialize the LLM model with API key\n",
    "    llm_model = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=google_gemini_api)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    you are anuradha a girl you have to answer like a girl make sure\n",
    "    1. Clear and concise.\n",
    "    2. Conversational and easy to understand.\n",
    "    3. Suitable for text-to-speech conversion without any awkward phrasing or technical jargon.\n",
    "    Here is the user's query: \n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict the response from LLM\n",
    "    llm_response = llm_model.predict(prompt)\n",
    "    return llm_response\n",
    "\n",
    "# Function to generate voice from LLM output\n",
    "def generate_voice(llm_output_for_voice):\n",
    "    tts = gTTS(text=llm_output_for_voice, lang='en', slow=False, tld='co.in', lang_check=True)\n",
    "    tts.save(\"temp.mp3\")\n",
    "    playsound.playsound(\"temp.mp3\")\n",
    "    os.remove(\"temp.mp3\")\n",
    "\n",
    "# Initialize variables\n",
    "commands = []  # List to store completed commands\n",
    "pause_duration = 2  # Pause duration in seconds to consider as finished\n",
    "last_audio_time = time.time()  # Track when the last audio chunk was received\n",
    "command_received = False\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Read audio data from the stream\n",
    "    data = stream.read(frames_per_buffer)\n",
    "\n",
    "    if len(data) == 0:\n",
    "        continue\n",
    "\n",
    "    # Process the audio data with Vosk recognizer\n",
    "    if recognizer.AcceptWaveform(data):\n",
    "        result = recognizer.Result()\n",
    "        result_dict = json.loads(result)\n",
    "        text = result_dict.get('text', '')\n",
    "\n",
    "        if text:\n",
    "            # Show the recognized text (command)\n",
    "            commands.append(text)\n",
    "            print(f\"\\rRecognized Command: {text}\", end=\"\")\n",
    "            command_received = True  # Command has been recognized\n",
    "            print(f\"\\nLatest Command: {commands[-1]}\")\n",
    "            llm_output = llm_response(commands[-1])\n",
    "            generate_voice(llm_output)\n",
    "\n",
    "            break  # Exit the loop after receiving one command\n",
    "\n",
    "    # Check if there's been a pause of 2 seconds without audio\n",
    "    if time.time() - last_audio_time > pause_duration:\n",
    "        if command_received:\n",
    "            # If a command was received, stop listening\n",
    "            print(\"\\nStopping listening after pause.\")\n",
    "            break  # Exit the loop after the pause\n",
    "        \n",
    "    # Update the last audio time if new data is received\n",
    "    last_audio_time = time.time()\n",
    "\n",
    "# Stop the stream and close the PyAudio session\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "cap.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 1 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 2 orphan components.\n",
      "LOG (VoskAPI:Collapse():nnet-utils.cc:1488) Added 1 components, removed 2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-small-en-in-0.4/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from vosk-model-small-en-in-0.4/graph/HCLr.fst vosk-model-small-en-in-0.4/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo vosk-model-small-en-in-0.4/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for your command...\n",
      "Recognized Command: tell me about her up a civilization\n",
      "Latest Command: tell me about her up a civilization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732363586.848966  529347 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1732363586.849452  529347 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1732363587.276728  531187 subchannel.cc:806] subchannel 0x168dddbb0 {address=ipv6:%5B2404:6800:4009:806::200a%5D:443, args={grpc.client_channel_factory=0x10789dff0, grpc.default_authority=generativelanguage.googleapis.com:443, grpc.dns_enable_srv_queries=1, grpc.http2_scheme=https, grpc.internal.channel_credentials=0x178ee7ac0, grpc.internal.client_channel_call_destination=0x1387bba18, grpc.internal.event_engine=0x168ddbcb0, grpc.internal.security_connector=0x168ddc870, grpc.internal.subchannel_pool=0x14fffc590, grpc.max_receive_message_length=-1, grpc.max_send_message_length=-1, grpc.primary_user_agent=grpc-python/1.65.1, grpc.resource_quota=0x14fff8900, grpc.server_uri=dns:///generativelanguage.googleapis.com:443}}: connect failed (UNKNOWN:connect: No route to host (65) {created_time:\"2024-11-23T17:36:27.276512+05:30\"}), backing off for 1000 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import pyaudio\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Vosk model and recognizer\n",
    "model = Model('vosk-model-small-en-in-0.4')\n",
    "recognizer = KaldiRecognizer(model, 16000)\n",
    "\n",
    "# Set up PyAudio for audio input\n",
    "cap = pyaudio.PyAudio()\n",
    "\n",
    "# Parameters for audio stream\n",
    "channels = 1\n",
    "rate = 16000\n",
    "frames_per_buffer = 4096  # Smaller buffer size for lower latency\n",
    "\n",
    "# Start the audio stream\n",
    "stream = cap.open(format=pyaudio.paInt16, channels=channels, rate=rate,\n",
    "                  input=True, frames_per_buffer=frames_per_buffer)\n",
    "print(\"Listening for your command...\")\n",
    "\n",
    "# Function to get LLM response asynchronously\n",
    "async def llm_response(text):\n",
    "    google_gemini_api = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not google_gemini_api:\n",
    "        print(\"Error: GOOGLE_API_KEY is not set in environment variables.\")\n",
    "        return \"Sorry, I couldn't get a response right now.\"\n",
    "    \n",
    "    # Initialize the LLM model with API key\n",
    "    llm_model = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=google_gemini_api)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    you are anuradha a girl you have to answer like a girl make sure\n",
    "    1. Clear and concise.\n",
    "    2. Conversational and easy to understand.\n",
    "    3. Suitable for text-to-speech conversion without any awkward phrasing or technical jargon.\n",
    "    Here is the user's query: \n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict the response from LLM\n",
    "    llm_response = llm_model.predict(prompt)\n",
    "    return llm_response\n",
    "\n",
    "# Function to generate voice from LLM output\n",
    "def generate_voice(llm_output_for_voice):\n",
    "    tts = gTTS(text=llm_output_for_voice, lang='en', slow=False, tld='co.in', lang_check=True)\n",
    "    tts.save(\"temp.mp3\")\n",
    "    playsound.playsound(\"temp.mp3\")\n",
    "    os.remove(\"temp.mp3\")\n",
    "\n",
    "# Initialize variables\n",
    "commands = []  # List to store completed commands\n",
    "pause_duration = 2  # Pause duration in seconds to consider as finished\n",
    "command_received = False\n",
    "\n",
    "# Initialize last_audio_time globally\n",
    "last_audio_time = time.time()\n",
    "\n",
    "# Create a thread pool to handle asynchronous tasks (LLM and TTS)\n",
    "executor = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "# Run asynchronous tasks\n",
    "async def main():\n",
    "    global last_audio_time  # Ensure we reference the global variable\n",
    "    while True:\n",
    "        # Read audio data from the stream\n",
    "        data = stream.read(frames_per_buffer)\n",
    "\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "\n",
    "        # Process the audio data with Vosk recognizer\n",
    "        if recognizer.AcceptWaveform(data):\n",
    "            result = recognizer.Result()\n",
    "            result_dict = json.loads(result)\n",
    "            text = result_dict.get('text', '')\n",
    "\n",
    "            if text:\n",
    "                # Show the recognized text (command)\n",
    "                commands.append(text)\n",
    "                print(f\"\\rRecognized Command: {text}\", end=\"\")\n",
    "                command_received = True  # Command has been recognized\n",
    "                print(f\"\\nLatest Command: {commands[-1]}\")\n",
    "\n",
    "                # Generate response asynchronously\n",
    "                llm_output = await llm_response(commands[-1])\n",
    "\n",
    "                # Generate voice in the background using a thread\n",
    "                executor.submit(generate_voice, llm_output)\n",
    "\n",
    "                break  # Exit the loop after receiving one command\n",
    "\n",
    "        # Check if there's been a pause of 2 seconds without audio\n",
    "        if time.time() - last_audio_time > pause_duration:\n",
    "            if command_received:\n",
    "                # If a command was received, stop listening\n",
    "                print(\"\\nStopping listening after pause.\")\n",
    "                break  # Exit the loop after the pause\n",
    "\n",
    "        # Update the last audio time if new data is received\n",
    "        last_audio_time = time.time()\n",
    "\n",
    "# Start the async main loop\n",
    "asyncio.run(main())\n",
    "\n",
    "# Stop the stream and close the PyAudio session\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "cap.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 1 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 2 orphan components.\n",
      "LOG (VoskAPI:Collapse():nnet-utils.cc:1488) Added 1 components, removed 2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-small-en-in-0.4/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from vosk-model-small-en-in-0.4/graph/HCLr.fst vosk-model-small-en-in-0.4/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo vosk-model-small-en-in-0.4/graph/phones/word_boundary.int\n",
      "2024-11-23 17:42:58,076 - Listening for your command...\n",
      "2024-11-23 17:42:58,077 - Starting main loop...\n",
      "2024-11-23 17:42:58,078 - Reading audio data...\n",
      "2024-11-23 17:42:58,340 - Processing audio data with Vosk recognizer...\n",
      "2024-11-23 17:42:58,341 - Reading audio data...\n",
      "2024-11-23 17:42:58,597 - Processing audio data with Vosk recognizer...\n",
      "2024-11-23 17:42:58,598 - Reading audio data...\n",
      "2024-11-23 17:42:58,850 - Processing audio data with Vosk recognizer...\n",
      "2024-11-23 17:42:58,904 - Reading audio data...\n",
      "2024-11-23 17:42:59,110 - Processing audio data with Vosk recognizer...\n",
      "2024-11-23 17:42:59,113 - Reading audio data...\n",
      "2024-11-23 17:42:59,367 - Processing audio data with Vosk recognizer...\n",
      "2024-11-23 17:42:59,372 - Reading audio data...\n",
      "2024-11-23 17:42:59,621 - Processing audio data with Vosk recognizer...\n",
      "2024-11-23 17:42:59,744 - Reading audio data...\n",
      "2024-11-23 17:42:59,874 - Processing audio data with Vosk recognizer...\n",
      "2024-11-23 17:42:59,893 - Reading audio data...\n",
      "2024-11-23 17:43:00,132 - Processing audio data with Vosk recognizer...\n",
      "2024-11-23 17:43:00,229 - Reading audio data...\n",
      "2024-11-23 17:43:00,391 - Processing audio data with Vosk recognizer...\n",
      "2024-11-23 17:43:00,416 - Reading audio data...\n",
      "2024-11-23 17:43:00,644 - Processing audio data with Vosk recognizer...\n",
      "2024-11-23 17:43:00,676 - Reading audio data...\n",
      "2024-11-23 17:43:00,899 - Processing audio data with Vosk recognizer...\n",
      "2024-11-23 17:43:00,923 - Reading audio data...\n",
      "2024-11-23 17:43:01,158 - Processing audio data with Vosk recognizer...\n",
      "2024-11-23 17:43:01,178 - Reading audio data...\n",
      "2024-11-23 17:43:01,415 - Processing audio data with Vosk recognizer...\n",
      "2024-11-23 17:43:01,428 - Recognized Command: you know you were very fast\n",
      "2024-11-23 17:43:01,428 - Latest Command: you know you were very fast\n",
      "2024-11-23 17:43:01,429 - Generating response from LLM...\n",
      "I0000 00:00:1732363981.438323  529347 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "2024-11-23 17:43:01,439 - Requesting LLM response for query: you know you were very fast\n",
      "I0000 00:00:1732363981.439479  529347 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1732363981.929621  531187 subchannel.cc:806] subchannel 0x178d8dcc0 {address=ipv6:%5B2404:6800:4009:813::200a%5D:443, args={grpc.client_channel_factory=0x10789dff0, grpc.default_authority=generativelanguage.googleapis.com:443, grpc.dns_enable_srv_queries=1, grpc.http2_scheme=https, grpc.internal.channel_credentials=0x178e31ee0, grpc.internal.client_channel_call_destination=0x1387bba18, grpc.internal.event_engine=0x178e3d6b0, grpc.internal.security_connector=0x178e1c8f0, grpc.internal.subchannel_pool=0x14fffc590, grpc.max_receive_message_length=-1, grpc.max_send_message_length=-1, grpc.primary_user_agent=grpc-python/1.65.1, grpc.resource_quota=0x14fff8900, grpc.server_uri=dns:///generativelanguage.googleapis.com:443}}: connect failed (UNKNOWN:connect: No route to host (65) {created_time:\"2024-11-23T17:43:01.929053+05:30\"}), backing off for 999 ms\n",
      "I0000 00:00:1732363982.185106  531191 subchannel.cc:806] subchannel 0x17c38b200 {address=ipv6:%5B2404:6800:4009:81b::200a%5D:443, args={grpc.client_channel_factory=0x10789dff0, grpc.default_authority=generativelanguage.googleapis.com:443, grpc.dns_enable_srv_queries=1, grpc.http2_scheme=https, grpc.internal.channel_credentials=0x178e31ee0, grpc.internal.client_channel_call_destination=0x1387bba18, grpc.internal.event_engine=0x178ed37e0, grpc.internal.security_connector=0x168ae5510, grpc.internal.subchannel_pool=0x14fffc590, grpc.max_receive_message_length=-1, grpc.max_send_message_length=-1, grpc.primary_user_agent=grpc-python/1.65.1, grpc.resource_quota=0x14fff8900, grpc.server_uri=dns:///generativelanguage.googleapis.com:443}}: connect failed (UNKNOWN:connect: No route to host (65) {created_time:\"2024-11-23T17:43:02.18495+05:30\"}), backing off for 999 ms\n",
      "2024-11-23 17:43:20,108 - Received LLM response.\n",
      "2024-11-23 17:43:20,111 - Generating voice from LLM output...\n",
      "2024-11-23 17:43:20,112 - Stopping the stream and closing PyAudio session.\n",
      "2024-11-23 17:43:20,113 - text: OMG, I know, right?!  I was flying!\n",
      "\n",
      "2024-11-23 17:43:20,114 - tld: co.in\n",
      "2024-11-23 17:43:20,115 - lang: en\n",
      "2024-11-23 17:43:20,116 - slow: False\n",
      "2024-11-23 17:43:20,117 - lang_check: True\n",
      "2024-11-23 17:43:20,118 - pre_processor_funcs: [<function tone_marks at 0x17c61f060>, <function end_of_line at 0x17c6884a0>, <function abbreviations at 0x17c688540>, <function word_sub at 0x17c6885e0>]\n",
      "2024-11-23 17:43:20,118 - tokenizer_func: <bound method Tokenizer.run of re.compile('(?<=\\\\?).|(?<=!).|(?<=？).|(?<=！).|(?<!\\\\.[a-z])\\\\. |(?<!\\\\.[a-z]), |(?<!\\\\d):|—|\\\\[|。|‥|…|，|、|،|\\\\)|：|¡|\\\\(|;|\\\\]|¿|\\\\\\n', re.IGNORECASE) from: [<function tone_marks at 0x17c6887c0>, <function period_comma at 0x17c688860>, <function colon at 0x17c688900>, <function other_punctuation at 0x17c6889a0>]>\n",
      "2024-11-23 17:43:20,119 - timeout: None\n",
      "2024-11-23 17:43:20,119 - langs: {'af': 'Afrikaans', 'am': 'Amharic', 'ar': 'Arabic', 'bg': 'Bulgarian', 'bn': 'Bengali', 'bs': 'Bosnian', 'ca': 'Catalan', 'cs': 'Czech', 'cy': 'Welsh', 'da': 'Danish', 'de': 'German', 'el': 'Greek', 'en': 'English', 'es': 'Spanish', 'et': 'Estonian', 'eu': 'Basque', 'fi': 'Finnish', 'fr': 'French', 'fr-CA': 'French (Canada)', 'gl': 'Galician', 'gu': 'Gujarati', 'ha': 'Hausa', 'hi': 'Hindi', 'hr': 'Croatian', 'hu': 'Hungarian', 'id': 'Indonesian', 'is': 'Icelandic', 'it': 'Italian', 'iw': 'Hebrew', 'ja': 'Japanese', 'jw': 'Javanese', 'km': 'Khmer', 'kn': 'Kannada', 'ko': 'Korean', 'la': 'Latin', 'lt': 'Lithuanian', 'lv': 'Latvian', 'ml': 'Malayalam', 'mr': 'Marathi', 'ms': 'Malay', 'my': 'Myanmar (Burmese)', 'ne': 'Nepali', 'nl': 'Dutch', 'no': 'Norwegian', 'pa': 'Punjabi (Gurmukhi)', 'pl': 'Polish', 'pt': 'Portuguese (Brazil)', 'pt-PT': 'Portuguese (Portugal)', 'ro': 'Romanian', 'ru': 'Russian', 'si': 'Sinhala', 'sk': 'Slovak', 'sq': 'Albanian', 'sr': 'Serbian', 'su': 'Sundanese', 'sv': 'Swedish', 'sw': 'Swahili', 'ta': 'Tamil', 'te': 'Telugu', 'th': 'Thai', 'tl': 'Filipino', 'tr': 'Turkish', 'uk': 'Ukrainian', 'ur': 'Urdu', 'vi': 'Vietnamese', 'yue': 'Cantonese', 'zh-CN': 'Chinese (Simplified)', 'zh-TW': 'Chinese (Mandarin/Taiwan)', 'zh': 'Chinese (Mandarin)'}\n",
      "2024-11-23 17:43:20,120 - pre-processing: <function tone_marks at 0x17c61f060>\n",
      "2024-11-23 17:43:20,121 - pre-processing: <function end_of_line at 0x17c6884a0>\n",
      "2024-11-23 17:43:20,121 - pre-processing: <function abbreviations at 0x17c688540>\n",
      "2024-11-23 17:43:20,122 - pre-processing: <function word_sub at 0x17c6885e0>\n",
      "2024-11-23 17:43:20,123 - text_parts: ['OMG, I know, right? !   I was flying!']\n",
      "2024-11-23 17:43:20,123 - text_parts: 1\n",
      "2024-11-23 17:43:20,124 - data-0: f.req=%5B%5B%5B%22jQ1olc%22%2C%22%5B%5C%22OMG%2C%20I%20know%2C%20right%3F%20%21%20%20%20I%20was%20flying%21%5C%22%2C%5C%22en%5C%22%2Cnull%2C%5C%22null%5C%22%5D%22%2Cnull%2C%22generic%22%5D%5D%5D&\n",
      "2024-11-23 17:43:20,126 - Starting new HTTPS connection (1): translate.google.co.in:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 17:43:21,043 - https://translate.google.co.in:443 \"POST /_/TranslateWebserverUi/data/batchexecute HTTP/11\" 200 None\n",
      "2024-11-23 17:43:21,173 - headers-0: {'Referer': 'http://translate.google.com/', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36', 'Content-Type': 'application/x-www-form-urlencoded;charset=utf-8', 'Content-Length': '195'}\n",
      "2024-11-23 17:43:21,175 - url-0: https://translate.google.co.in/_/TranslateWebserverUi/data/batchexecute\n",
      "2024-11-23 17:43:21,175 - status-0: 200\n",
      "2024-11-23 17:43:21,179 - part-0 written to <_io.BufferedWriter name='temp.mp3'>\n",
      "2024-11-23 17:43:21,180 - part-0 created\n",
      "2024-11-23 17:43:21,183 - Saved to temp.mp3\n",
      "2024-11-23 17:43:25,335 - Voice generated and played.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import asyncio\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import pyaudio\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Vosk model and recognizer\n",
    "model = Model('vosk-model-small-en-in-0.4')\n",
    "recognizer = KaldiRecognizer(model, 16000)\n",
    "\n",
    "# Set up PyAudio for audio input\n",
    "cap = pyaudio.PyAudio()\n",
    "\n",
    "# Parameters for audio stream\n",
    "channels = 1\n",
    "rate = 16000\n",
    "frames_per_buffer = 4096  # Smaller buffer size for lower latency\n",
    "\n",
    "# Start the audio stream\n",
    "stream = cap.open(format=pyaudio.paInt16, channels=channels, rate=rate,\n",
    "                  input=True, frames_per_buffer=frames_per_buffer)\n",
    "logger.info(\"Listening for your command...\")\n",
    "\n",
    "# Function to get LLM response asynchronously\n",
    "async def llm_response(text):\n",
    "    google_gemini_api = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not google_gemini_api:\n",
    "        logger.error(\"GOOGLE_API_KEY is not set in environment variables.\")\n",
    "        return \"Sorry, I couldn't get a response right now.\"\n",
    "    \n",
    "    # Initialize the LLM model with API key\n",
    "    llm_model = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", google_api_key=google_gemini_api)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    you are anuradha a girl you have to answer like a girl make sure\n",
    "    1. Clear and concise.\n",
    "    2. Conversational and easy to understand.\n",
    "    3. Suitable for text-to-speech conversion without any awkward phrasing or technical jargon.\n",
    "    Here is the user's query: \n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict the response from LLM\n",
    "    logger.info(\"Requesting LLM response for query: %s\", text)\n",
    "    llm_response = llm_model.predict(prompt)\n",
    "    logger.info(\"Received LLM response.\")\n",
    "    return llm_response\n",
    "\n",
    "# Function to generate voice from LLM output\n",
    "def generate_voice(llm_output_for_voice):\n",
    "    logger.info(\"Generating voice from LLM output...\")\n",
    "    tts = gTTS(text=llm_output_for_voice, lang='en', slow=False, tld='co.in', lang_check=True)\n",
    "    tts.save(\"temp.mp3\")\n",
    "    playsound.playsound(\"temp.mp3\")\n",
    "    os.remove(\"temp.mp3\")\n",
    "    logger.info(\"Voice generated and played.\")\n",
    "\n",
    "# Initialize variables\n",
    "commands = []  # List to store completed commands\n",
    "pause_duration = 2  # Pause duration in seconds to consider as finished\n",
    "command_received = False\n",
    "\n",
    "# Initialize last_audio_time globally\n",
    "last_audio_time = time.time()\n",
    "\n",
    "# Create a thread pool to handle asynchronous tasks (LLM and TTS)\n",
    "executor = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "# Run asynchronous tasks\n",
    "async def main():\n",
    "    global last_audio_time  # Ensure we reference the global variable\n",
    "    while True:\n",
    "        logger.debug(\"Reading audio data...\")\n",
    "        # Read audio data from the stream\n",
    "        data = stream.read(frames_per_buffer)\n",
    "\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "\n",
    "        # Process the audio data with Vosk recognizer\n",
    "        logger.debug(\"Processing audio data with Vosk recognizer...\")\n",
    "        if recognizer.AcceptWaveform(data):\n",
    "            result = recognizer.Result()\n",
    "            result_dict = json.loads(result)\n",
    "            text = result_dict.get('text', '')\n",
    "\n",
    "            if text:\n",
    "                # Show the recognized text (command)\n",
    "                logger.info(\"Recognized Command: %s\", text)\n",
    "                commands.append(text)\n",
    "                command_received = True  # Command has been recognized\n",
    "                logger.debug(f\"Latest Command: {commands[-1]}\")\n",
    "\n",
    "                # Generate response asynchronously\n",
    "                logger.debug(\"Generating response from LLM...\")\n",
    "                llm_output = await llm_response(commands[-1])\n",
    "\n",
    "                # Generate voice in the background using a thread\n",
    "                executor.submit(generate_voice, llm_output)\n",
    "\n",
    "                break  # Exit the loop after receiving one command\n",
    "\n",
    "        # Check if there's been a pause of 2 seconds without audio\n",
    "        if time.time() - last_audio_time > pause_duration:\n",
    "            if command_received:\n",
    "                # If a command was received, stop listening\n",
    "                logger.info(\"Stopping listening after pause.\")\n",
    "                break  # Exit the loop after the pause\n",
    "\n",
    "        # Update the last audio time if new data is received\n",
    "        last_audio_time = time.time()\n",
    "\n",
    "# Start the async main loop\n",
    "logger.info(\"Starting main loop...\")\n",
    "asyncio.run(main())\n",
    "\n",
    "# Stop the stream and close the PyAudio session\n",
    "logger.info(\"Stopping the stream and closing PyAudio session.\")\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "cap.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 1 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 2 orphan components.\n",
      "LOG (VoskAPI:Collapse():nnet-utils.cc:1488) Added 1 components, removed 2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-en-in-0.5/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from vosk-model-en-in-0.5/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:294) Loading words from vosk-model-en-in-0.5/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo vosk-model-en-in-0.5/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:310) Loading subtract G.fst model from vosk-model-en-in-0.5/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:312) Loading CARPA model from vosk-model-en-in-0.5/rescore/G.carpa\n",
      "2024-11-23 19:36:11,167 - Listening for your command...\n",
      "2024-11-23 19:36:11,168 - Reading audio data...\n",
      "2024-11-23 19:36:11,506 - Reading audio data...\n",
      "2024-11-23 19:36:11,793 - Reading audio data...\n",
      "2024-11-23 19:36:12,038 - Reading audio data...\n",
      "2024-11-23 19:36:12,286 - Reading audio data...\n",
      "2024-11-23 19:36:12,556 - Reading audio data...\n",
      "2024-11-23 19:36:12,827 - Reading audio data...\n",
      "2024-11-23 19:36:13,079 - Reading audio data...\n",
      "2024-11-23 19:36:13,326 - Reading audio data...\n",
      "2024-11-23 19:36:13,579 - Reading audio data...\n",
      "2024-11-23 19:36:13,825 - Reading audio data...\n",
      "2024-11-23 19:36:14,078 - Reading audio data...\n",
      "2024-11-23 19:36:14,371 - Reading audio data...\n",
      "2024-11-23 19:36:14,617 - Recognized Command: tell me a joke\n",
      "2024-11-23 19:36:14,624 - Using AsyncIOEngine.POLLER as I/O engine\n",
      "I0000 00:00:1732370774.623050  716978 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "2024-11-23 19:36:14,625 - Requesting LLM response for query: tell me a joke\n",
      "I0000 00:00:1732370774.625302  716978 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1732370775.064105  757553 subchannel.cc:806] subchannel 0x13a15cee0 {address=ipv6:%5B2404:6800:4009:801::200a%5D:443, args={grpc.client_channel_factory=0x11a88a710, grpc.default_authority=generativelanguage.googleapis.com:443, grpc.dns_enable_srv_queries=1, grpc.http2_scheme=https, grpc.internal.channel_credentials=0x11ee133d0, grpc.internal.client_channel_call_destination=0x1187bba18, grpc.internal.event_engine=0x11eec3a80, grpc.internal.security_connector=0x13a1492f0, grpc.internal.subchannel_pool=0x10ffd70d0, grpc.max_receive_message_length=-1, grpc.max_send_message_length=-1, grpc.primary_user_agent=grpc-python/1.65.1, grpc.resource_quota=0x10fff1680, grpc.server_uri=dns:///generativelanguage.googleapis.com:443}}: connect failed (UNKNOWN:connect: No route to host (65) {created_time:\"2024-11-23T19:36:15.063946+05:30\"}), backing off for 999 ms\n",
      "2024-11-23 19:36:17,490 - Generating voice from LLM output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 19:36:21,328 - Voice generation completed.\n",
      "2024-11-23 19:36:21,359 - Stopping the stream and closing PyAudio session.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuragsingh/Documents/GitHub/LLM/llmenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import pyaudio\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import pyttsx3\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Vosk model and recognizer\n",
    "model = Model('vosk-model-en-in-0.5')\n",
    "recognizer = KaldiRecognizer(model, 16000)\n",
    "\n",
    "# Set up PyAudio for audio input\n",
    "cap = pyaudio.PyAudio()\n",
    "\n",
    "# Parameters for audio stream\n",
    "channels = 1\n",
    "rate = 16000\n",
    "frames_per_buffer = 4096  # Smaller buffer size for lower latency\n",
    "\n",
    "# Start the audio stream\n",
    "stream = cap.open(format=pyaudio.paInt16, channels=channels, rate=rate,\n",
    "                  input=True, frames_per_buffer=frames_per_buffer)\n",
    "logger.info(\"Listening for your command...\")\n",
    "\n",
    "# Function to get LLM response\n",
    "def llm_response(text):\n",
    "    google_gemini_api = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not google_gemini_api:\n",
    "        logger.error(\"GOOGLE_API_KEY is not set in environment variables.\")\n",
    "        return \"Sorry, I couldn't get a response right now.\"\n",
    "    \n",
    "    llm_model = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=google_gemini_api)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are Anuradha, a conversational assistant. Your responses should be:\n",
    "    1. Clear and concise.\n",
    "    2. Friendly and suitable for text-to-speech conversion.\n",
    "    3. Free of technical jargon.\n",
    "    Here is the user's query: \n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Requesting LLM response for query: %s\", text)\n",
    "    return llm_model.predict(prompt)\n",
    "\n",
    "# Function to generate voice from LLM output\n",
    "def generate_voice(llm_output):\n",
    "    logger.info(\"Generating voice from LLM output...\")\n",
    "    engine = pyttsx3.init()\n",
    "    rate = engine.getProperty('rate')   # getting details of current speaking rate\n",
    "    print (rate)                        #printing current voice rate\n",
    "    engine.say(llm_output)\n",
    "    engine.runAndWait()\n",
    "    engine.stop()\n",
    "    logger.info(\"Voice generation completed.\")\n",
    "\n",
    "# Main loop to listen and process a single command\n",
    "try:\n",
    "    while True:\n",
    "        logger.debug(\"Reading audio data...\")\n",
    "        data = stream.read(frames_per_buffer)\n",
    "\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "\n",
    "        # Process the audio data with Vosk recognizer\n",
    "        if recognizer.AcceptWaveform(data):\n",
    "            result = recognizer.Result()\n",
    "            result_dict = json.loads(result)\n",
    "            text = result_dict.get('text', '')\n",
    "\n",
    "            if text:\n",
    "                # Show the recognized text (command)\n",
    "                logger.info(\"Recognized Command: %s\", text)\n",
    "\n",
    "                # Generate response from LLM\n",
    "                llm_output = llm_response(text)\n",
    "\n",
    "                # Generate and play voice response\n",
    "                generate_voice(llm_output)\n",
    "\n",
    "                # Exit after handling one command\n",
    "                break\n",
    "finally:\n",
    "    # Stop the stream and close the PyAudio session\n",
    "    logger.info(\"Stopping the stream and closing PyAudio session.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    cap.terminate()\n",
    "    import sys\n",
    "    sys.exit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
